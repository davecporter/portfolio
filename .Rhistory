library(tidyverse)
library(MASS)
select <- dplyr::select
library(boot)
library(ISLR)
library(mlbench)
library(GGally)
source(file.path(getwd(), 'classification_model_helpers.R'))
View(compare_class_method)
library(tidyverse)
library(MASS)
select <- dplyr::select
library(boot)
library(ISLR)
library(mlbench)
library(GGally)
source(file.path(getwd(), 'classification_model_helpers.R'))
library(tidyverse)
library(MASS)
select <- dplyr::select
library(boot)
library(ISLR)
library(mlbench)
library(GGally)
source(file.path(getwd(), 'classification_model_helpers.R'))
k_test <- 5
auto_preds <- Auto %>%
mutate(mpg01 = ifelse(mpg > median(mpg), 1, 0) %>% as.factor()) %>%
select(-name, -mpg)
compare_class_method(auto_preds, "mpg01", ".", n_trials=500, k=k_test)
View(compare_class_method)
compare_class_method(auto_preds, "mpg01", ".", n_trials=500, k=k_test)
require(devtools)
install_version("tidyverse", version = "1.2.1", repos = "http://cran.us.r-project.org")
require(devtools)
install.packages(devtools)
install.packages("devtools")
library(devtools)
install.packages("devtools")
install.packages("devtools")
install_version("gtidyverse", version = "1.2.1", repos = "http://cran.us.r-project.org")
require(devtools)
install_version("gtidyverse", version = "1.2.1", repos = "http://cran.us.r-project.org")
install_version("tidyverse", version = "1.2.1", repos = "http://cran.us.r-project.org")
remove.packages("tidyverse")
install_version("tidyverse", version = "1.2.1", repos = "http://cran.us.r-project.org")
library(tidyverse)
library(ISLR)
library(tidyverse)
library(ISLR)
library(tidyverse)
library(ISLR)
library(tidyverse)
library(ISLR)
remove.packages("markdown")
install_version("tmarkdown", version = "0.8", repos = "http://cran.us.r-project.org")
require(devtools)
install_version("tmarkdown", version = "0.8", repos = "http://cran.us.r-project.org")
install_version("markdown", version = "0.8", repos = "http://cran.us.r-project.org")
remove.packages("rmarkdown")
install_version("rmarkdown", version = "1.6", repos = "http://cran.us.r-project.org")
install_version("rmarkdown", version = "1.6", repos = "http://cran.us.r-project.org")
library(tidyverse)
library(ISLR)
# odds vs probability
plotr <- data.frame(p_X = seq(0,0.99,0.01)) %>% mutate(odds = p_X / (1 - p_X))
ggplot(plotr, aes(p_X, odds)) + geom_line()
summary(glm(default ~ balance, family = binomial, data=Default))
b0 <- -10.65
b1 <- 5.5e-3
logistic_function <- function(b0, b1, X) { # (eq 4.2)
e <- exp(b0 + b1 * X)
e / (1 + e)
}
plotr <- data.frame(balance=Default$balance, P_default=logistic_function(b0=b0, b1=b1, Default$balance))
ggplot(plotr, aes(balance, P_default)) + geom_line() # (equivalent to fig 4.2)
exp(b1) # (eq 4.3)
odds <- 4
new_odds <- odds * exp(b1)
delta_odds <- new_odds - odds
delta_odds
(new_odds / (1 + new_odds)) - (odds / (1 + odds))
X_at_odds <- (log(odds) - b0) / b1
X_at_odds
logistic_function(b0, b1, X_at_odds + 1) - logistic_function(b0, b1, X_at_odds)
logistic_regression <- function(nA, meanA=0, sdA=1, nB, meanB=0, sdB=1,
p_cutoff=0.5, seed=round(runif(1)*1e6), show_conf_matrix=TRUE){
set.seed(seed)
# initialise data, cl = class, p1d = probability density
df <- data.frame(X = c(rnorm(nA, meanA, sdA), rnorm(nB, meanB, sdB)), cl = c(rep("A", nA), rep("B", nB))) %>%
mutate(p1d = ifelse(cl == "A", dnorm(X, meanA, sdA), dnorm(X, meanB, sdB)))
# fit logistic regression model and extract coefficients
logistic_reg <- glm(cl ~ X, data=df, family = "binomial")
summ_log_reg <- summary(logistic_reg)
b0 <- logistic_reg$coefficients[1]
b1 <- logistic_reg$coefficients[2]
p_val <- summ_log_reg$coefficients[2,4]
# calculate probabilities and classify predictions using logistic function
df <- df %>% mutate(p_X = logistic_function(b0, b1, X),
pred_cl = ifelse(p_X < p_cutoff, "A", "B"))
# create confusion matrix
conf_matrix <- table(df %>% select(pred_cl, cl))
prop_matrix_row <- prop.table(conf_matrix, margin=1)
prop_matrix_col <- prop.table(conf_matrix, margin=2)
accuracy <- mean(df$pred_cl == df$cl)
precision <- prop_matrix_row[1,1] # positive predictive value
sensitivity <- prop_matrix_col[1,1] # true positive rate
specificity <- prop_matrix_col[2,2] # true negative rate
annotation <- paste(paste("p value =", round(p_val, 2)),
paste("p(X) cutoff =", p_cutoff),
paste("accuracy =", round(accuracy, 2)),
paste("precision =", round(precision, 2)),
paste("true A rate =", round(sensitivity, 2)),
paste("true B rate =", round(specificity, 2)),
sep = "\n")
if(show_conf_matrix==TRUE) print(conf_matrix)
# graphical output
ggplot(df, aes(X, col=cl)) +
geom_rug(alpha=0.1) + # data points
geom_density(aes(y=..count../max(..count..))) + # density of data point counts
geom_density(aes(y=..count.., fill=cl), position="fill", alpha=0.1, linetype=2) + # prob. area of data points
geom_line(aes(X, p_X), colour="black") + # logistic function of data points
geom_rug(aes(X, col=pred_cl), alpha=0.1, sides="t") + # predictions
annotate("text", x=min(df$X)*0.5, y=0.8, label=annotation) +
labs(y="normalised count & p(X)")
}
logistic_regression(nA=10000, nB=10000, meanB=1)
install_version("stats", version = "3.3.3", repos = "http://cran.us.r-project.org")
install_version("stats", version = "3.3.3", repos = "R-core R-core@R-project.org")
R.Version()
R.Version()
require(devtools)
install.packages("devtools")
require(devtools)
install_version("tidyverse", version = "1.2.1", repos = "http://cran.us.r-project.org")
install.packages("devtools")
install_version("tidyverse", version = "1.2.1", repos = "http://cran.us.r-project.org")
require(devtools)
library(dplyr)
library(ISLR)
# odds vs probability
plotr <- data.frame(p_X = seq(0,0.99,0.01)) %>% mutate(odds = p_X / (1 - p_X))
ggplot(plotr, aes(p_X, odds)) + geom_line()
library(dplyr)
library(ggplot2)
library(ISLR)
# odds vs probability
plotr <- data.frame(p_X = seq(0,0.99,0.01)) %>% mutate(odds = p_X / (1 - p_X))
ggplot(plotr, aes(p_X, odds)) + geom_line()
summary(glm(default ~ balance, family = binomial, data=Default))
b0 <- -10.65
b1 <- 5.5e-3
logistic_function <- function(b0, b1, X) { # (eq 4.2)
e <- exp(b0 + b1 * X)
e / (1 + e)
}
plotr <- data.frame(balance=Default$balance, P_default=logistic_function(b0=b0, b1=b1, Default$balance))
ggplot(plotr, aes(balance, P_default)) + geom_line() # (equivalent to fig 4.2)
exp(b1) # (eq 4.3)
odds <- 4
new_odds <- odds * exp(b1)
delta_odds <- new_odds - odds
delta_odds
(new_odds / (1 + new_odds)) - (odds / (1 + odds))
X_at_odds <- (log(odds) - b0) / b1
X_at_odds
logistic_function(b0, b1, X_at_odds + 1) - logistic_function(b0, b1, X_at_odds)
logistic_regression <- function(nA, meanA=0, sdA=1, nB, meanB=0, sdB=1,
p_cutoff=0.5, seed=round(runif(1)*1e6), show_conf_matrix=TRUE){
set.seed(seed)
# initialise data, cl = class, p1d = probability density
df <- data.frame(X = c(rnorm(nA, meanA, sdA), rnorm(nB, meanB, sdB)), cl = c(rep("A", nA), rep("B", nB))) %>%
mutate(p1d = ifelse(cl == "A", dnorm(X, meanA, sdA), dnorm(X, meanB, sdB)))
# fit logistic regression model and extract coefficients
logistic_reg <- glm(cl ~ X, data=df, family = "binomial")
summ_log_reg <- summary(logistic_reg)
b0 <- logistic_reg$coefficients[1]
b1 <- logistic_reg$coefficients[2]
p_val <- summ_log_reg$coefficients[2,4]
# calculate probabilities and classify predictions using logistic function
df <- df %>% mutate(p_X = logistic_function(b0, b1, X),
pred_cl = ifelse(p_X < p_cutoff, "A", "B"))
# create confusion matrix
conf_matrix <- table(df %>% select(pred_cl, cl))
prop_matrix_row <- prop.table(conf_matrix, margin=1)
prop_matrix_col <- prop.table(conf_matrix, margin=2)
accuracy <- mean(df$pred_cl == df$cl)
precision <- prop_matrix_row[1,1] # positive predictive value
sensitivity <- prop_matrix_col[1,1] # true positive rate
specificity <- prop_matrix_col[2,2] # true negative rate
annotation <- paste(paste("p value =", round(p_val, 2)),
paste("p(X) cutoff =", p_cutoff),
paste("accuracy =", round(accuracy, 2)),
paste("precision =", round(precision, 2)),
paste("true A rate =", round(sensitivity, 2)),
paste("true B rate =", round(specificity, 2)),
sep = "\n")
if(show_conf_matrix==TRUE) print(conf_matrix)
# graphical output
ggplot(df, aes(X, col=cl)) +
geom_rug(alpha=0.1) + # data points
geom_density(aes(y=..count../max(..count..))) + # density of data point counts
geom_density(aes(y=..count.., fill=cl), position="fill", alpha=0.1, linetype=2) + # prob. area of data points
geom_line(aes(X, p_X), colour="black") + # logistic function of data points
geom_rug(aes(X, col=pred_cl), alpha=0.1, sides="t") + # predictions
annotate("text", x=min(df$X)*0.5, y=0.8, label=annotation) +
labs(y="normalised count & p(X)")
}
logistic_regression(nA=10000, nB=10000, meanB=1)
logistic_regression(nA=5000, nB=3000, meanB=1, sdB=0.5, seed = 687435)
logistic_regression(nA=5000, nB=3000, meanB=1, sdB=0.5, seed = 687435, p_cutoff = 0.4)
logistic_regression(nA=5000, nB=1000, meanB=3, sdB=1, seed = 67312)
logistic_regression(nA=5000, nB=1000, meanB=3, sdB=1, seed = 67312, p_cutoff = 0.2)
library(dplyr)
library(tidyr)
library(stringr)
library(ggplot2)
library(class)
# seed for random data generation
global_seed <- 21789 # runif(1)*1e6)
# number of training observations
n_train <- 100
# number of test observations
n_test <- 10
# group A normal distribution mean and standard deviation
gpA_mu_sd <- c(-0.5, 1)
# groupd B normal distribution mean and standard deviation
gpB_mu_sd <- c(0.5, 1)
set.seed(global_seed)
gpA = data.frame(gp = "A",
p1 = rnorm(n_train, mean=gpA_mu_sd[1], sd=gpA_mu_sd[2]),
p2 = rnorm(n_train, mean=gpA_mu_sd[1], sd=gpA_mu_sd[2]),
stringsAsFactors = FALSE)
gpB = data.frame(gp = "B",
p1 = rnorm(n_train, mean=gpB_mu_sd[1], sd=gpB_mu_sd[2]),
p2 = rnorm(n_train, mean=gpB_mu_sd[1], sd=gpB_mu_sd[2]),
stringsAsFactors = FALSE)
train = bind_rows(gpA, gpB)
# ggplot(train, aes(p1, p2, col=gp)) + geom_point()
set.seed(global_seed + 1) # +1 enables generation of different test data to train
test_Xp <- data.frame(p1 = rnorm(n_test, mean=-0.5), p2 = rnorm(n_test, mean=-0.5), stringsAsFactors = FALSE)
groups <- data.frame(gp = paste("y_hat", seq(1, n_test), sep = ""), stringsAsFactors = FALSE)
test <- cbind(test_Xp, groups)
knn_func <- function(train, test, k, seed=round(runif(1)*1e6)){
# calculate euclidean distances
distances <- dist(rbind(test, train)) %>% as.matrix()
colnames(distances) <- rownames(distances) <- c(test[["gp"]], train[["gp"]])
n_test <- nrow(test)
# selects nearest k neighbours
knn_predict <- function(){
set.seed(seed) # used to randomly select ties
distances[,1:n_test] %>% as.data.frame() %>% mutate(gp = rownames(.)) %>%
gather("y_hat", "dist", contains("y_hat")) %>%
filter(!str_detect(gp, "y_hat")) %>%
group_by(y_hat) %>%
top_n(-k) %>%
# randomly choose between ties
count(gp) %>%
mutate(n = n + runif(1)) %>%
filter(n == max(n)) %>%
mutate(n = round(n))
}
# run `knn_predict()` and put predictions in same order as `test` input
knn_pred <- knn_predict() %>% select(-n) %>%
mutate(pred_no = str_replace_all(y_hat, "y_hat", "") %>% as.numeric()) %>% arrange(pred_no)
plotr <- left_join(knn_pred, test %>% rename(y_hat = gp))
plot <- ggplot() + geom_point(data = train, aes(p1, p2, col=gp), shape=1) +
geom_point(data = plotr, aes(p1, p2, col=gp), size=2)
list(knn_pred %>% pull(gp), plot)
}
knn_func_results <- knn_func(train, test, k=5)
knn_func_results
set.seed(global_seed)
train_X <- train %>% select(p1, p2)
train_y <- train %>% pull(gp)
knn_results <- knn(train_X, test[,-3], train_y, 5)
data.frame(knn_func_results[[1]], knn_results) %>% mutate(same_result = .[,1] == .[,2])
install.packages("devtools")
require(devtools)
install.packages(install_version("dplyr", version = "0.8.0.1", repos = "http://cran.us.r-project.org"))
install.packages(install_version("dplyr", version = "0.8.0.1", repos = "http://cran.us.r-project.org"))
library(dplyr)
library(tidyr)
install.packages(install_version("dplyr", version = "0.8.2", repos = "http://cran.us.r-project.org"))
require(devtools)
install.packages(install_version("dplyr", version = "0.8.2", repos = "http://cran.us.r-project.org"))
install.packages(install_version("dplyr", version = "0.8.2", repos = "http://cran.us.r-project.org"))
library(dplyr)
library(tidyr)
library(stringr)
library(ggplot2)
library(class)
# seed for random data generation
global_seed <- 21789 # runif(1)*1e6)
# number of training observations
n_train <- 100
# number of test observations
n_test <- 10
# group A normal distribution mean and standard deviation
gpA_mu_sd <- c(-0.5, 1)
# groupd B normal distribution mean and standard deviation
gpB_mu_sd <- c(0.5, 1)
set.seed(global_seed)
gpA = data.frame(gp = "A",
p1 = rnorm(n_train, mean=gpA_mu_sd[1], sd=gpA_mu_sd[2]),
p2 = rnorm(n_train, mean=gpA_mu_sd[1], sd=gpA_mu_sd[2]),
stringsAsFactors = FALSE)
gpB = data.frame(gp = "B",
p1 = rnorm(n_train, mean=gpB_mu_sd[1], sd=gpB_mu_sd[2]),
p2 = rnorm(n_train, mean=gpB_mu_sd[1], sd=gpB_mu_sd[2]),
stringsAsFactors = FALSE)
train = bind_rows(gpA, gpB)
# ggplot(train, aes(p1, p2, col=gp)) + geom_point()
set.seed(global_seed + 1) # +1 enables generation of different test data to train
test_Xp <- data.frame(p1 = rnorm(n_test, mean=-0.5), p2 = rnorm(n_test, mean=-0.5), stringsAsFactors = FALSE)
groups <- data.frame(gp = paste("y_hat", seq(1, n_test), sep = ""), stringsAsFactors = FALSE)
test <- cbind(test_Xp, groups)
knn_func <- function(train, test, k, seed=round(runif(1)*1e6)){
# calculate euclidean distances
distances <- dist(rbind(test, train)) %>% as.matrix()
colnames(distances) <- rownames(distances) <- c(test[["gp"]], train[["gp"]])
n_test <- nrow(test)
# selects nearest k neighbours
knn_predict <- function(){
set.seed(seed) # used to randomly select ties
distances[,1:n_test] %>% as.data.frame() %>% mutate(gp = rownames(.)) %>%
gather("y_hat", "dist", contains("y_hat")) %>%
filter(!str_detect(gp, "y_hat")) %>%
group_by(y_hat) %>%
top_n(-k) %>%
# randomly choose between ties
count(gp) %>%
mutate(n = n + runif(1)) %>%
filter(n == max(n)) %>%
mutate(n = round(n))
}
# run `knn_predict()` and put predictions in same order as `test` input
knn_pred <- knn_predict() %>% select(-n) %>%
mutate(pred_no = str_replace_all(y_hat, "y_hat", "") %>% as.numeric()) %>% arrange(pred_no)
plotr <- left_join(knn_pred, test %>% rename(y_hat = gp))
plot <- ggplot() + geom_point(data = train, aes(p1, p2, col=gp), shape=1) +
geom_point(data = plotr, aes(p1, p2, col=gp), size=2)
list(knn_pred %>% pull(gp), plot)
}
knn_func_results <- knn_func(train, test, k=5)
knn_func_results
set.seed(global_seed)
train_X <- train %>% select(p1, p2)
train_y <- train %>% pull(gp)
knn_results <- knn(train_X, test[,-3], train_y, 5)
data.frame(knn_func_results[[1]], knn_results) %>% mutate(same_result = .[,1] == .[,2])
